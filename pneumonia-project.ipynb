{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T06:27:24.755999Z","iopub.execute_input":"2025-06-14T06:27:24.756298Z","iopub.status.idle":"2025-06-14T06:27:37.584579Z","shell.execute_reply.started":"2025-06-14T06:27:24.756270Z","shell.execute_reply":"2025-06-14T06:27:37.583454Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomRotation(10),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/input/chest-xray-pneumonia/chest_xray'  # Update this path\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val', 'test']}\n\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32,\n                             shuffle=True, num_workers=4)\n              for x in ['train', 'val', 'test']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\nclass_names = image_datasets['train'].classes\n\nprint(f\"Dataset sizes: {dataset_sizes}\")\nprint(f\"Class names: {class_names}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_sample_images(dataloader, class_names, num_images=8):\n    \"\"\"Display sample images from the dataset\"\"\"\n    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n    axes = axes.ravel()\n    \n    dataiter = iter(dataloader)\n    images, labels = next(dataiter)\n    \n    for i in range(num_images):\n        img = images[i]\n        # Denormalize\n        img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        img = torch.clamp(img, 0, 1)\n        \n        axes[i].imshow(img.permute(1, 2, 0))\n        axes[i].set_title(f'Class: {class_names[labels[i]]}')\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_sample_images(dataloaders['train'], class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(model_name='resnet50', num_classes=2, freeze_features=True):\n    \"\"\"\n    Create a pre-trained model for transfer learning\n    \n    Args:\n        model_name: 'resnet50', 'densenet121', or 'efficientnet_b0'\n        num_classes: number of output classes\n        freeze_features: whether to freeze feature extraction layers\n    \"\"\"\n    if model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n        \n        if freeze_features:\n            for param in model.parameters():\n                param.requires_grad = False\n            # Only train the classifier\n            for param in model.fc.parameters():\n                param.requires_grad = True\n                \n    elif model_name == 'densenet121':\n        model = models.densenet121(pretrained=True)\n        num_ftrs = model.classifier.in_features\n        model.classifier = nn.Linear(num_ftrs, num_classes)\n        \n        if freeze_features:\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.classifier.parameters():\n                param.requires_grad = True\n                \n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(pretrained=True)\n        num_ftrs = model.classifier[1].in_features\n        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n        \n        if freeze_features:\n            for param in model.parameters():\n                param.requires_grad = False\n            for param in model.classifier.parameters():\n                param.requires_grad = True\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=10):\n    \"\"\"Train the model with early stopping and metrics tracking\"\"\"\n    train_losses = []\n    train_accs = []\n    val_losses = []\n    val_accs = []\n    \n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    patience = 5\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n        \n        # Each epoch has training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n            \n            running_loss = 0.0\n            running_corrects = 0\n            \n            # Progress bar\n            pbar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch+1}')\n            \n            for inputs, labels in pbar:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                # Update progress bar\n                pbar.set_postfix({\n                    'loss': f'{loss.item():.4f}',\n                    'acc': f'{torch.sum(preds == labels.data).float()/inputs.size(0):.4f}'\n                })\n            \n            if phase == 'train':\n                scheduler.step()\n            \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            \n            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            \n            # Store metrics\n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accs.append(epoch_acc.cpu().numpy())\n            else:\n                val_losses.append(epoch_loss)\n                val_accs.append(epoch_acc.cpu().numpy())\n                \n                # Early stopping and best model saving\n                if epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = model.state_dict()\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    \n                if patience_counter >= patience:\n                    print(f'Early stopping triggered after {epoch + 1} epochs')\n                    break\n        \n        print()\n        \n        if patience_counter >= patience:\n            break\n    \n    print(f'Best val Acc: {best_acc:4f}')\n    model.load_state_dict(best_model_wts)\n    \n    return model, {\n        'train_losses': train_losses,\n        'train_accs': train_accs,\n        'val_losses': val_losses,\n        'val_accs': val_accs\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_training_history(history):\n    \"\"\"Plot training history\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot loss\n    ax1.plot(history['train_losses'], label='Training Loss')\n    ax1.plot(history['val_losses'], label='Validation Loss')\n    ax1.set_title('Model Loss')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    \n    # Plot accuracy\n    ax2.plot(history['train_accs'], label='Training Accuracy')\n    ax2.plot(history['val_accs'], label='Validation Accuracy')\n    ax2.set_title('Model Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_loader, class_names):\n    \"\"\"Evaluate model on test set\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc='Testing'):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Classification report\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n    \n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n    \n    # Calculate test accuracy\n    test_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    \n    return all_preds, all_labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training...\")\nprint(f\"Model: ResNet50 (frozen features)\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\n# Train the model\ntrained_model, history = train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=20)\n\n# Plot training history\nplot_training_history(history)\n\n# Evaluate on test set\nprint(\"\\nEvaluating on test set...\")\ntest_preds, test_labels = evaluate_model(trained_model, dataloaders['test'], class_names)\n\n# Save the model\ntorch.save(trained_model.state_dict(), 'pneumonia_classifier.pth')\nprint(\"Model saved as 'pneumonia_classifier.pth'\")\n\n# Example of fine-tuning (unfreezing some layers)\nprint(\"\\n=== Fine-tuning Phase ===\")\nprint(\"Unfreezing last few layers for fine-tuning...\")\n\n# Unfreeze the last residual block\nfor param in trained_model.layer4.parameters():\n    param.requires_grad = True\n\n# Lower learning rate for fine-tuning\noptimizer = optim.Adam(trained_model.parameters(), lr=0.0001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n# Fine-tune for a few more epochs\nfinetuned_model, ft_history = train_model(trained_model, dataloaders, criterion, optimizer, scheduler, num_epochs=10)\n\n# Final evaluation\nprint(\"\\nFinal evaluation after fine-tuning...\")\nfinal_preds, final_labels = evaluate_model(finetuned_model, dataloaders['test'], class_names)\n\n# Save final model\ntorch.save(finetuned_model.state_dict(), 'pneumonia_classifier_finetuned.pth')\nprint(\"Fine-tuned model saved as 'pneumonia_classifier_finetuned.pth'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}